## 第三章
+ 推論にはニューラルネットワークを用いる
+ そこで、有名なword2vecが登場してくるのですが、その仕組みをじっくり見ていこう

+ 推論ベース手法とニューラルネットネットワーク
    + 「カウントベースの手法」と「推論ベースの手法」は単語をベクトルで表す研究では成功している事例である
    + 両者とも異なるアプローチをしていますが、分布仮説を背景に考えている
    + ニューラルネットワークで「単語」を処理する例を見ていきます
    + カウントベースの手法の問題点
         + カウントベース手法では、周囲の単語の頻度によって単語を表現している
             + 単語の共起行列を作り、その行列に対してSVD を適用することで、密なベクトル――単語の分散表現――を獲得した
         + コーパスで扱う語彙数は非常に巨大になればなるほど、巨大な行列計算に計算コストがかかってしまう
　　　　 + 1回の処理で単語の分散表現を獲得しますが、推論ベース手法ではニューラルネットワークを用いるためミニバッチで学習するのが一般的です
　　　　 + ニューラルネットワークは一度に少量(ミニバッチ)の学習サンプルを見ながら、重みを繰り返し更新するということです
    + 推論ベースの手法の概要
         + 「推論」することが主な作業になります
         + 周囲の単語が与えらた時、「?」にどの単語がくるのかを推論する作業
         + 例) you ? goodbye and i say hello.
         + このような推論問題をたくさん繰り返し解くことで、単語の出力パターンを学習する
　　+ ニューラルネットワークにおける単語の処理方法
         + ニューラルネットワークで単語を処理するには、それを「固定長のベクトル」に変換する必要がある
　　　　 + その方法の一つとして、one-hot表現があげられる
　　　　 + 先ほど利用していた1文をコーパスとしてするなら、「you」-> [1 0 0 0 0 0 0], 「goodbye」-> [0 0 1 0 0 0 0]
         + そして、入力層のニューロン数は7つで、7つのニューロン共に1つの単語が当てまはるようになる
　　　　 + 中間層のニューロンは、入力層のニューロンとの重み付き和となる
         + 例となるコードは下に書いておく
```
import numpy as np
c = np.array([[1, 0, 0, 0, 0, 0, 0]]) # 入力
W = np.random.randn(7, 3) # 重み
h = np.dot(c, W) # 中間ノード 
print(h)
```
+ シンプルなword2vec     
    + word2vecで提案されている手法の一つが、continuous bag-of-words(CBOW)である
    + CBOW モデルの推論処理
        + コンテキストからターゲットを推測することを目的としたニューラルネットワークです(コンテキスト -> 「周囲の単語」、ターゲット -> 「中央の単語)
        + CBOWモデルを使い、正確な推測を行えるように訓練を行うことが目標である
        + CBOWモデルの入力層は、2つあり中間層を経て出力層へとたどり着きます
　　　　+ 入力層から中間層への変換は、同じ結合層(重みWin)によって行われ、中間層から出力層のニューロンの変換は、別の結合層(重みWout)によって行われる
        + 中間層にあるニューロンは、各入力層の全結合による変換後の値が「平均」されたものになります
        + そして、出力層には7つのニューロンがあり、それぞれの単語に対応している
        + 出力層のニューロンは、各単語の「スコア」であり、softmax関数を用いることで「確率」を得られる
	+ 重みWinは、7*3の行列となっており、これが単語の分散表現の正体となっている
        + そして、Winの各行にはそれぞれの単語の分散表現を格納されていて、学習を行う度に各単語の分散表現が更新されていき、得られたベクトルが「単語の意味となる」
　　　　+ ポイントとしては、中間層のニューロン数が入力層よりも減らすことが重要で、単語を予測するために必要な情報を “コンパクト” に収める必要があり、結果として密なベクトル表現が得られる

    + CBOW モデルの学習
        + “良い重み” のネットワークがあれば、「確率」を表すニューロンにおいて、正解に対応するニューロンが高くなっていることが期待できます
        + CBOW モデルの学習で行うことは、正しい予測ができるように重みを調整すること
        + コーパスが違えば、学習で得られる単語の分散表現も異なります
　　　　+ コーパスとして「スポーツ」の記事だけを使う場合と「音楽」の記事だけを使う場合とでは、得られる単語の分散表現は大きく異なる 
        + これから扱うモデルは、多クラス分類を行うニューラルネットワークです(Softmax と交差エントロピー誤差を用いる)
        + softmaxレイヤとCross Entropy Errorrレイヤを用いたニューラルネットワークの順伝搬になり、2つのレイヤを足し合わせたレイヤのことをSoftmax with Lossレイヤと呼ぶ
　　+ word2vec の重みと分散表現
        + word2vecのニューラルネットワークには、WinとWoutの2つの重みが存在し、Winの各行には各単語の分散表現に対応し、Woutも単語の意味がエンコードされたベクトルが格納されている
        + Winの各行に単語の意味が格納され、Woutの列方向に単語の意味が格納されている
        + word2vec(skip-gramモデル)では、「入力層で使われた重みだけを利用する」のがポピュラーである
        + skip-gramでは、入力層の重みを使い良い結果を出したと報告があり、GloVeでは、入力層と出力層両方を取り入れて良い結果を得たと言う報告が上がっている
 + word2vec学習データの準備
    +  今回も「You say goodbye and I say hello.」と言う一文をコーパスとして利用する
    + コンテキストとデータ
　　　　+ word2vecのニューラルネットワークの入力層では「コンテキスト(周囲の単語)」を入力として、正解ラベル「ターゲット(中央の単語)」とする
        + コーパスの中から「ターゲット」と「コンテキスト」を抜き出します　なお、ターゲットは両端の単語を入れない
 + CBOWのモデル実装
    + 実際にモデル作成して、実装してみたら分散表現をうまく捉えているように思えた
 + word2vecに関する補足
    + CBOWモデルを「確率」の視点から考えてみる
    + CBOWモデルと確率
        + Aと言う事象が起こる確率をP(A)と呼ぶ
        + 同時確率とは、「Aと言う事象とBと言う事象が同時に起こる確率」のことを言い、P(A,B)と書く
        + 事後確率とは、P(A|B)と書き、「事が怒った後の確率」または「B(と言う情報)が与えられた時、Aが起こる確率」
　　　  + CBOWモデルが行うことは、コンテキストを与えるとターゲットとなる単語の確率を出力する
　　　　+ ここで、単語の列を W1,W2,W3,...,Wt,Wt+1 与えられたとした時に、ターゲットとなる単語をWtとした時の事後確率を求める
        + P(Wt|Wt-1,Wt+1) -> 「Wt-1とWt+1が起こった時、Wtが起こる確率」
　　　　+ CBOWモデルの損失関数も簡潔に書ける -> 交差エントロピー誤差の式は、L = -sigm(tlog(y)) yは事象が起こる確率、tは教師ラベル
        + 「Wtが起こる事象」が正解であるため、それに対応するone-hotベクトルの要素が 1 で、それ以外は 0 になります
　　　　+ L = - logP(Wt|Wt-1,Wt+1) 先程の確率にlogを取り、マイナスをつけた負の対数尤度となる
　　　　+ 先ほどの式は、サンプルデータに関する損失関数であるため、コーパス全体に拡張した損失関数は
        + L = - (1/T)sigm(logP(Wt|Wt-1,Wt+1))となる
        + この時の重みパラメータが私たちが目的とする単語の分散表現となる
    + skip-gram
        + コンテキストとターゲットを逆転させたモデルである
　　　　+ skip-gramは、中央の単語(ターゲット)から周囲の単語(コンテキスト)を推測する手法である
        + 先ほどの事後確率を用いて考えれば、P(Wt-1,Wt+1|Wt)となる
　　　　+ この式は、「Wtが与えられたとき、Wt-1とWt+1が同時に起こる確率」
　　　　+ この式を「条件付き独立」として次のように分解する
        + P(Wt-1,Wt+1|Wt) = P(Wt-1|Wt)P(Wt+1|Wt)となる
　　　　+ この式を交差エントロピー誤差を適用すると、skip-gramモデルの損失関数を求められる  L = -logP(Wt-1, Wt+1|Wt) = - logP(Wt-1|Wt)P(Wt+1|Wt) = - (logP(Wt-1|Wt) + logP(Wt+1|Wt))
        + コーパス全体に拡張すると、L = -(1/T)sigm(logP(Wt-1|Wt) + logP(Wt+1|Wt))
        + skip-gram モデルはコンテキストの数だけ推測をするため、その損失関数は各コンテキストで求めた損失の総和を求める必要があります
        + 単語の分散表現の精度の点において、多くの場合、skip-gram モデルのほうが良い結果が得られている
　　　　+ コーパスが大規模になるにつれて、低頻出の単語や類推問題の性能の点において、skip-gram モデルのほうが優れた結果が得られる傾向にあります
        + 学習速度の点では、CBOW モデルのほうがskip-gram モデルよりも高速 (コンテキストの数だけ損失を求めるため)
     + 推論ベースVSカウントベース
　　　　+ ミニバッチ処理による学習コストや、新しい単語の追加による再学習もパラメータを変化させることなく行えるため、推論ベースはかなり有用だと思われる
        + カウントベース手法は単語の類似性をエンコードされ、推論ベース手法は単語の類似性の他に複雑な単語間のパターンも捉えることができる
        + 重要な事実として、推論ベースの手法とカウントベースの手法には、関連性があることが分かっています
        + 推論ベースとカウントベースの手法を融合させたようなGloVeと言う手法も提案されている
　　　　+ コーパス全体の統計データの情報を損失関数に取り入れミニバッチ学習をすることにあります 
 
+ まとめ
    + ニューラルネットワークを利用して、類似性のある単語や複雑な単語間のパターンも捉えることができた
    + skip-gramとCBOWの両者の違いを明確に指摘し、どちらも有効性と得意としないパターンがある
　　+ word2vec の転移学習の有用性についてまとめていた

## 第四章
+ CBOWモデルは、シンプルな2層のニューラルネットワークであるため、実装自体は難しくはない
+ しかし、コーパスで取り扱う語彙数の増大することによって、計算時間が増えてしまう欠点を持つ
+ 本章では、word2vecの高速化を目的とします
    + 1つめに、Embedding レイヤという新しいレイヤを導入すること
    + 2つめに、Negative Sampling という新しい損失関数を導入
### word2vecの改良1
+ 例えば、語彙数が100万、中間層のニューロン数が100の場合のモデルを想定する
    + 入力層と出力層に100万のニューロンがあり途中計算で膨大な時間がかかってしまう
        + 入力層のone-hot表現と重み行列Winの積による計算
            + 語彙数が100万となってしまったら、one-hot表現も100万の要素を占めるメモリサイズになってしまう
            + one-hotのベクトルとWinの重み行列との積の計算を行わなくてはならないため、膨大な計算リソースが必要になる
                + 解決案： Embedding レイヤ を新たに導入する
        + 中間層と重み行列Woutの積およびSoftmaxレイヤの計算
            + 中間層と重み行列Woutの積で多くの時間がかかってしまう
            + oftmax レイヤに関わる箇所でも、扱う語 彙数が増えるにつれて計算量が増加することが問題になります
                + 解決案：egative Sampling という新しい損失関数を導入する

+ Embeddingレイヤ(単語をone-hot表現の場合には使えそう)
    + 前章で行った行列計算は、単に行列の特定の行を抜き出すことだけ
    + そのため、one-hot表現への変換とMatMulレイヤでの行列の乗算は必要なさそう
    + それでは、重みパラメータから「単語IDに該当する行(ベクトル)」を抜き出すためのレイヤを作りましょう
    + 本章では、そのレイヤをEmbeddingレイヤと呼ぶ
    + 順伝播は、重みWの特定の行だけを抽出している
    + これは単に、重みの特定の行のニューロ ンだけを――何の手も加えず――次の層へと流したことになります
　　+ 逆伝播では、前の層(出力側の層)から伝わってきた勾配を次の層(入力側の層)へそのまま流すだけになります 
    + 前層から伝わる勾配を、重みの勾配 dW の特定の行(idx)に設定するようにします
    + backward()の実装
        + 重みW と同じ大きさの行列 dW を作成し、そのdW の該当する行に勾配を代入しました
        + 重みWの更新を行いため、dWは作る必要がない
　　　　+ 更新したい行番号(idx)とその勾配(dout)を保持しておけば、その情報から重み(W)の特定の行だけを更新することができる
        + 現状のbackward()には、idxの要素が重複した時に問題が発生する
        + 現状だと、同じ要素を指定した時に、「代入」の処理を行ってしまうため、せっかくの値が上書きされてしまう
        + 「代入」から「加算」と言う処理に切り換えて行う
        + dW[...] = 0 と言う処理は、Wを0にするのではなく、dW の形状を保ったまま、その要素を0にします

### word2vec の改良2
+ 次のボトルネックな処理としては、中間層以降の処理ー行列の積とSoftmaxレイヤの計算
+ Negative Sampling(負例サンプリング)と呼ばれる手法を用います
+ softmax -> Negative sampling　を行うだけで彙数がどれだけ多くなったとしても、計算量を少なく一定に抑えることができる  

+ 中間層以降の計算の問題
    + 入力層と出力層には100 万個のニューロン,　中間層のニューロンは100と指定する
    + 中間層のニューロンと重み行列(Wout)の積 (中間層と出力層の巨大な行列の積)
    + Softmax レイヤの計算 (softmaxの式から、分母のexp計算を100万回行わないといけない)
+ 多値分類から二値分類へ
    + Negative sampling という手法のキーとなるアイディアは、「二値分類」にある
　  + より正確な説明をすると、「多値分類」を「二値分類」で近似する
	+ 例)多値分類 -> 「コンテキストが『you』と『goodbye』のとき、ターゲットとなる単語は何ですか?」
        + 例)二値分類 -> 「コンテキストが『you』と『goodbye』のとき、ターゲットとなる単語は『say』 ですか?」
        + そうした場合、出力層にはニューロンをひとつだけ用意すれば事足ります
        + 「say」であるスコアを出力すれば良い
        + つまり、中間層の出力と「say」に対応する列ベクトルを計算するば良い
　　　　+ 中間層と出力側の重み行列の積は、「say」に対応する列(単語ベクトル)だけを抽出し、その抽出したベクトルと中間層のニューロンとの内積を計算すればよいことになります
        + 出力側の重み Wout では、各単語 ID の単語ベクトルが各列 に格納されています
        + 「say」という単語ベクトルを抽出し、そのベクトルと中間層のニューロンとの内積を求めます

+ シグモイド関数と交差エントロピー誤差
    + 二値分類をニューラルネットワークで解く際には、スコアに「シグモイド関数」、損失関数には「交差エントロピー誤差」を使用する
    + 多値分類の場合は、出力層には「ソフトマックス関数」、損失関数には「交差エントロピー誤差」
    + 式(ソフトマックス関数)：y = 1 / (1 + exp(-x))
    + 損失関数は、確率yを得たらこの確率yの損失関数を求める
    + 式(交差エントロピー誤差)：L = -( (t)log(y) + (1-t)log(1-y) )
    + y はシグモイド関数の出力、t は正解ラベルとし、この正解ラベルt は、0 か1のどちらかの値を取ります
　　+ t が1のときは正解が「Yes」、tが0のときは正解が「No」です


+ Negative Sampling
    + これだけでは問題が解決したは言い切れません
    + 今のままでは、正例(正しい答え)についてだけ学習を行ったに過ぎないから
    + 負例(誤った答え)については、どのような結果になるのか定かではありません (正解問題しかやっていないため、間違いに対してはどのような振る舞いをするかわからない)
    + 我々がやるべきことは、正例 (「say」)についてはSigmoid レイヤの出力を1に近づけ、負例(「say」以外の単語) については、Sigmoidレイヤの出力を0に近づける
　  + 多値分類の問題を二値分類として扱うためには、「正しい答え(正例)」と「間違った答え(負例)」のそれぞれに対して、正しく(二値)分類できる必要がある　　
　　+ つまり、ネガティブサンプリングとは、それぞれのデータ(正例とサンプリングした負例)における損失を足し合わせて、最終的な損失とする



+ Negative Sampling のサンプリング手法
    + 負例をどのようにサンプリングするか？
　　+ コーパスの統計データに基づいて、サンプリングを行う
    + コーパス中でよく使われる単語は抽出されやすくし、コーパス中であまり使われない単語は抽出されにくくする
　　+ コーパスの中から、単語の使用回数をを求めこれを「確率分布」で表す
　　+ レア単語をNegative Samplingで用いらない理由として、現実問題その単語の利用頻度が少なく重要性が高いとは考えにくい、そのため高頻度で出現する単語をNegative Samplingに利用する
    + 元の確率分布に対して0.75累乗して、P(Wi)はi番目の単語の確率を表します
    + 元の確率分布の各要素を「0.75 乗」するだけ、ただし、変換後も確率の総和が1になるように、分母には「変換後の確率分布の総和」になる必要になります
    + 出現確率の低い単語を “見捨てない”ようにして、確率の低い単語に対してその確率を少しだけ高くすることができる

+ Negative Sampling の実装
    + 出力側の重みを表すW、コーパス(単語IDのリスト)のcorpus、確率分布の累乗の値をPower、そして負例のサンプリング数のsample_sizeを取る
    + UnigramSampler クラスを生成し、それをメンバ変数のsamplerで保持する
    + メンバ変数の loss_layers と embed_dot_layers

+ 改良版word2vecの学習 
    + 改良を取り入れたニューラルネットワークを実装を取り組む(EmbeddingレイヤとNegative Sampling)
   
    + CBOWモデルの実装
        + SimpleCBOW クラスを改良するために、EmbeddingレイヤとNegative Samplingレイヤを使う
        + 改良前:入力側の重みと出力側の重みの形状は異なっており、出力側の重みでは、列方向に単語ベクトルが配置されていた
　　　  + 一方 CBOW クラスの出力側の重みは、入力側の重みと同じ形状で、行方向に単語ベクトルが配置されている
	+ 今回のモデルは、ウィンドウサイズは5で隠れ層のニューロン数は100にせってしてあるが、大体は、ウィンドウサイズは2~10で隠れ層のニューロン数は50~500が成功率が高いと考えられる
　　　　+ コーパスがあまりにも大きいため処理に半日はかかったそう
        + 学習した重みデータは、学習した環境によって変わると考えられている
            + 重みの初期化に用いるランダムな初期値
            + ミニバッチを無作為に選ぶこと
            + Negative Samplingにおけるサンプリングのランダム性
　　　　　　+ 最終的に得られる重みは各自の環境で異なりますが、大きな視点で見ると、同じような結果(傾向)が得られるでしょう。

    + CBOW モデルの学習コード


    + CBOW モデルの評価
        + 学習した分散表現を確認した結果、単語の分散表現はかなり良い結果を示したと言える
        + word2vecは、より複雑なパターンを捉えることができ、類推問題をベクトルの加算と減算で解くことができる
　　　　+ これも試した結果、クトルの加減算で類推問題が解けることがわかった
　　　　+ しかし、これは筆者が良い結果を取るものを選んだだけにすぎず多くの問題において期待した結果を得られないかもしれない

+ word2vec に関する残りのテーマ
 
    + word2vec を使ったアプリケーションの例
	+ word2vecで得られた単語の分散表現は、類似単語を用途に利用できるだけでなく、転移学習にも使える
	+ 先に、大きなコーパスで学習を行い、学習済みの分散表現を個別のタスクで利用できる(テキスト分類、文章のクラスタリング、品詞のタグ付、感情分析など)
	+ 他の利点としては、単語を固定長のベクトルに変換できる
	+ さらに、文章でも固定長のベクトル表現を行える
	+ どのように文章を固定長のベクトルに変換するかは、盛んに研究されています
	+ 章の各単語を分散表現に変換し、それらの総和を求めること(bag-of-words) 単語の順序を考慮しないモデル
	+ 文章や単語を固定長ベクトルに変換できれば、様々な機械学習の手法が適応できる
	+ 然言語をベクトルに変換することによって、一般的な機械学習システムの枠組みで目的の答えを出力すること(そして、学習すること)が可能になる
	


　　+ 単語ベクトルの評価方法
	+ システム(たとえば感情分析を行うシステム)は、複数のシステムから構成される
	+ 複数というのは、単語の分散表現を作るシステム(word2vec)と特定の問題に対して分類を行うシステム(たとえば感情を分類するSVM など)
	+ 単語の分散表現の次元数が最終的な精度にどのように影響するかを調べるには、まず単語の分散表現の学習を行い、そしてその分散表現を使って、もうひとつの機械学習システムの学習を行わなければなりません
	+ 2 段階の学習を行った上で評価する必要がある
	+ パラメーターチューニングを2つのシステムで行う必要がある
	+ 現実的な評価をするには、単語の「類似性」や「類推問題」を用います
	+ 「類似性」とは、人間が作成した単語類似度の評価セットを用いる
	+ 「類推問題」は、単語の意味や文法的な問題を正しく理解しているか
	+ しかし、単語の文さ表現はの良さがアプリケーションにどれだけ貢献するかはアプリの種類やコーパスの内容、取り扱う問題によって変化する
	+ つまり、評価指標が高くても目的するアプリでも必ず良い結果になるかはわからない

+ まとめ



+  学んだこと
    + Embeddingレイヤは単語の分散表現を格納し、順伝搬において該当する単語IDのベクトル抽出する
    + word2vecでは語彙数の増加に比例して計算量が増加するので、近似計算を行う高速な手法を使う
    + Negative Samplingは負例をいくつかサンプリングする手法であり、これを利用するれば多値分類を二値分類として扱うことができる
    + word2vecによって得られた単語の分散表現は、単語の意味が埋め込まれたものであり、似たコンテキストで使われている単語は単語ベクトルの空間上で似た場所に近づける
    + word2vecの単語の分散表現は、類似問題をベクトルの加算と減算によって解ける性質を持つ
    + word2vecは、転移学習の点で特に重要であり、その単語の分散表現を様々な自然言語処理のタスクに利用する


## 第五章
+ 我々が見てきたニューラルネットワークは、フィードフォワードと呼ばれるタイプのネットワークです
+ 流れが一方向のネットワーク(入力信号が次の層(隠れ層)へ信号を伝達し、信号を受け取った層はまた信号を伝達する)
+ 単純な構成で理解がしやすいし、多くの問題に応用ができる
+ しかし、時系列データを取り扱うことが困難である
+ 単純なフィードフォワードでは、時系列データの性質を学習することができない
+ そこで、今章ではRNN(リカレントニューラルネットワーク)を使う

+ 確率と言語モデル
   + 自然言語に関する現象を「確率」を使って記述し、最後に、言語を確率として扱う「自然言語モデル」について説明する
   + word2vec を確率の視点から眺める
        + CBOWモデルは、周囲の単語(コンテキスト)を用いて、中央の単語(ターゲット)を求める手法を使う
	+ W1,W2,....,Wt-1,Wt,Wt+1,......と言う単語の並びがあり、コンテキスト(Wt-1,Wt+1)とターゲット(Wt)を用いて、P(Wt|Wt-1,Wt+1)となる
	+ この式が意味するのは「Wt-1とWt+1が与えられたとき、Wtが起きる確率」
	+ 左のウィンドウだけをコンテキストとして、考えられる式は P(Wt|Wt-1,Wt-2)となる
	+ この式を用いた損失関数は、交差エントロピー誤差を用いて、 L = -logP(Wt|Wt-1,Wt+1)となる
	+ コーパス全体の損失関数の総和をなるべく小さくする重みパラメータを見つけ、見つかることができれば、モデルはコンテキストからターゲットを正しく推測できるようになったと言える
	+ 学習を進めると、副産物として単語の意味をエンコードされた単語の分散表現を獲得することができる
	+ モデルの本来の目的である、「コンテキストからターゲットを推測すること」と先ほどのP(Wt|Wt-1,Wt-2)は何かに利用できないだろうか
 
   + 言語モデル
	+ 単語の並びに対して確率を与えることで、自然な単語の並びかどうかを評価する
	+ 例）「you say goodbye」-> 0.092　、　「you say good die」-> 0.0000000000032
	+ 利用用途としては、ある質問文に対する応答文候補として幾つかの文章を生成しておく、そして、言語モデルを利用して候補となる文章が「文章として自然であるかどうか」の基準でランクづけを行う
	+ また、文章生成にも利用ができ、確率分布に従って単語をサンプリングする
	+ 数式を使って表す、　W1,W2,W3,....,Wmの順序で単語が同時に出現する確率をP(W1,W2,W3,....,Wm)となる
	+ この同時確率P(W1,W2,W3,....,Wm)を事後確率を使って分解して求める
	+ P(W1,W2,W3,....,Wm) = P(Wm|W1,...,Wm-1)P(Wm-1|W1,...,Wm-2)...P(W3|W1,W2)P(W2|W1)P(W1) = PiP(Wt|W1,...,Wt-1)
	+ 同時確率は事後確率の総乗で表すことができ、その結果は確率の乗法定理から導く
	+ P(A,B) = P(A|B)P(B) -> 「A とB の両方が起こる確率P(A,B)」は、「B が起こる確率P (B)」と「B が起こった後にAが起こる確率P(A|B)」をかけ合わせたものになるということです
	+ 乗法定理を使えば、m個の単語の同時確率P(W1,W2,...,Wm)を事後確率　->　P(W1,w2,...,Wm) = P(A,Wm) = P(Wm|A)P(A)
	+ ここで、W1,...,Wm-1をまとめてAと言う記号で表す　->　A(W1,..,Wm-1)
	+ P(A) = P(W1,...,Wt-2,Wt-1) = P(A',Wm-1) = P(Wm-1|A')P(A') 後はこの手順を繰り返し行う、目的とする同時確率は事後確率の総乗によって表す
	+ P(Wt|W1,...,Wt-1)を表すモデルは、条件付き言語モデルと呼ぶ、P (wt |w1 , . . . , wt−1 ) を表すモデルを指して、それを「言語モデル」と呼ぶ

　 + CBOW モデルを言語モデルに?
	+ CBOWモデルを言語モデルに適用するには、コンテキストのサイズをある値に限定することで近似的に表すことができる
	+ P(W1,...,Wm) = Pi(Wt|w1,...,Wt-1) ~ Pi(Wt|Wt-2,Wt-1)
	+ コンテキストを左側の2つの単語に限定する
	+ 「マルコフ性」とは、未来の状態が現状の状態に依存だけに依存して決まる
	+ CBOWモデルのコンテキスト数が答えに必要な量ではない場合、正しく答えることができない
	+ CBOWモデルではコンテキストはいくらでも大きくできるが、コンテキスト内の単語の並びが無視されると言う問題がある
	+ コンテキストの単語の並びが無視されている問題の例として -> 2個の単語ベクトルの「和」が中間層にくる、そのためコンテキストの単語の並び方は無視される (you,say)を(say,you)が同じものと扱われている
	+ 単語の並びを考慮したモデルは、中間層において「連結」することが考えられる
	+ しかし、連結するアプローチをとったらコンテキストのサイズに比例して重みパラメータが増えることになり、そのようなパラメータの増加は歓迎されません
	+ ここで、登場するのはRNN(リカレントニューラルネットワーク)である
	+ RNNでは、コンテキストが長くても、そのコンテキストの情報を記憶するメカニズムを持っている
	+ RNN による言語モデルでも単語の分散表現を獲得できたのですが、語彙数増加への対応や単語の分散表現の “質” の向上のために、word2vecが提案された

   + RNNとは
	+ 「何度も繰り返し起こる」「再発する」「周期的に起こる」「循環する」、RNNとは「再発するニューラルネットワーク」と「循環するニューラルネットワーク」
	+ 循環するニューラルネットワーク
		+ 「閉じた経路」もしくは「ループした経路」そのような経路が存在することで初めて、媒体(もしくはデータ)は同じ場所を繰り返し行き来すること
		+ データがループすることで、情報は絶えず更新される
		+ RNN特徴は、ループする経路(閉じた経路)を持ち、データは絶えず循環することができ、データが循環することにより過去の情報を記憶しながら最新のデータへと更新される
		+ 時系列データとして、(X0,X1,...,Xt,...)のデータがレイヤへ入力され、その入力に対応するする形で(h0,h1,...,ht,...)が出力される
		+ 文章(単語の並び)を扱う場合、各単語の分散表現(単語ベクトル)をXt として、それをRNN レイヤへ入力する
	
	+ ループの展開
		+ RNNはのループ構造は通常のニューラルネットワークとは異なるが、ループを展開することで馴染みのニューラルネットワークになる
		+ 複数のRNNレイヤは、全て同じレイヤであることが、これまでのニューラルネットワークとは異なります
		+ 時系列データは、時間方向にデータが並び、そのため時系列データのインデックスを指すために「時刻」という言葉を使います
		+ 展開図を見てわかるように、各時刻の RNN レイヤは、そのレイヤへの入力とひとつ前の RNN レイヤからの出力を受け取ります
		+ 2 つの情報を元に、その時刻の出力が計算されます
		+ ht = tanh(ht-1 Wh + xt Wx + b)
		+ RNNには二つの重みが存在し、一つは入力Xを出力hに変換する重みWx、もう一つは一つの前のRNNの出力を次時刻の出力に変換する重みWhである、またバイアスbがある
		+ ht-1とXtは行ベクトル
		+ 行列の積を行い、それらの和をtanh関数(双曲線正接関数)によって変換する
		+ 時刻tの出力htは別のレイヤに向けて上方向へ出力され、次時刻のRNNレイヤに向けて右方向へも出力される
		+ 現在の出力は、一つ前の出力によって計算される、これが「状態を持つレイヤ」「メモリを持つレイヤ」と言われる
	
	+ Backpropagation Through Time
		+ RNNの学習も通常のネットワークと同じような手順で学習を行う
		+ 誤差逆伝播法は、「時間方向に展開したニューラルネットワークの誤差逆伝播法」Backpropagation Through Time -> BPTT
		+ 問題としては、長い時系列データを学習する場合に、BPTT で消費するコンピュータの計算のリソースも増加する
		+ また、時間サイズが長くなると、逆伝播時に勾配が不安定になることもある
	
	+ Truncated BPTT
		+ 大きな時系列データを扱う際に、ネットワークを適当な長さに'断ち切る'ことによって、小さなネットワークを複数作ることが目的となっている
		+ 小さなネットワークに対して、誤差逆伝播法を行うことが Truncated BPTTと呼ぶ
		+ Truncated BPTTは、「順伝播」の繋がりを維持し、「逆伝播」は断ち切りそのネットワーク内で学習を行う
		+ 例えば、逆伝播を10個単位で区切って学習を行えば、それより未来のデータについて考える必要がなくなり誤差逆伝播法を完結させることができる
		+ RNNの学習を行う際には、順伝播の繋がりを考慮して行い、データをシーケンシャルに与えると言うことだ
		+ 今までのネットワークでは、バッチ処理を行う時データをランダムに選んでいたがTruncated BPTT を行う場合、データは “シーケンシャル” に与える必要がある
		+ 一つ目のブロックの入力データをRNNレイヤに与えると、図5-12のようになる
		+ 次のブロックの学習が始まる前に、前のブロックの最後の隠れ状態であるデータ(h9)を渡し、順伝播の繋がりを維持している
		+ そして、図5-14からわかるようにTruncated BPTT ではデータをシーケンシャルに与えて学習を行えば順伝播を維持しながら、ブロック単位で誤差逆伝播を適用できる
	
	+ Truncated BPTT のミニバッチ学習
		+ 先ほどまでの学習をミニバッチ学習で言えば、バッチ数が1の時に相当する
		+ バッチを考慮に入れ、尚且つシーケンシャルにデータを与えるためには、データを与える開始位置を各バッチで「ズラす」必要がある
		+ 説明すると、1000この長さの時系列データに対して、時間の長さを10個単位で切るTruncated BPTTで学習する場合、バッチ数を2として学習する場合、一つ目のバッチは先頭から順にデータを与え、2つ目のバッチは500番目のデータを開始位置として、順にデータを与える必要がある
		+ つまり、開始位置を500だけ「ズラす」と言うことだ
		+ ミニバッチ学習を行うためには、バッチの開始位置をオフセットとしてズラして、シーケンシャルに与えていきます
		+ シーケンシャルにデータを与えていく途中で終端に達した場合は、先頭に戻すような対応が必要になります
		+  Truncated BPTT の原理は単純だが、「データをシーケンシャルに与える」と「各バッチでデータを与える開始位置をズラすこと」

    + RNNの実装
	+ RNNを実装するためには、横並びの固定サイズ分の一連のネットワークを構築すれば良い
	+ 図5-17を見れば、上下方向の入力と出力をそれぞれ一つのレイヤとして束ねたら一つのレイヤとして見ることができる
	+ つまり、xs = {x0,x1,...,xT-1}(入力)とhs = {h0,h1,...,hT-1}(出力)として見做せることが出来る
	+ Time RNN レイヤ内の 1 ステップの処理を行うレイヤを「RNN レイヤ」と呼び、T ステップ分の処理をまとめて行うレイヤを「Time RNN レイヤ」と呼ぶ
	+ RNN レイヤの実装
		+ 
	















