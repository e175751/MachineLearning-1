# ゼロから始めるDeepLearning2

## 第二章
### シソーラス手法
### カウントベース手法
### カウントベース手法
+ 概要
    + 自然言語処理の研究やアプリケーションのために目的を持った作られた、コーパス(大量のテキストデータ)を利用する
    + コーパスには、文章の書き方や単語の選び方、単語の意味、自然言語に対する知識がコーパスに含まれている
+ pythonによるコーパス準備
    + コーパスの準備をするために、WikipediaやGoogleNewsを用いる
    + コーパスの前処理を行う際には、テキストを単語に分割し、単語IDのリストして落とし込む
    + しかし、ここのままでは単語のリストのため扱うのは困難である
    + そこで、単語にIDを紐づけることによってIDのリストとすることで利用できるようにする
```
text = 'You say goodbye and I say hello.'
text = text.lower()
text = text.replace('.', ' .')
words = text.split(' ')
``` 
```
word_to_id = {}
id_to_word = {}
for word in words:
    if word not in word_to_id:
        new_id = len(word_to_id)
        word_to_id[word] = new_id
        id_to_word[new_if] = word
```
+ 単語の分散表現
    + 色を表す際に、RGBと言うベクトル表現を行う
    + 単語も同様に、コンパクトで理に適ったベクトル表現を行う
    + 「単語の意味」を的確に捉えたベクトル表現を単語の分散表現と呼ぶ
+ 分布仮説
    + 「単語の意味は、周辺の単語の意味によって形成される」
    + ここから、中央単語(目的の単語)を「ターゲット」と呼び、その周りの単語(周囲単語)のことを「コンテキスト」と呼ぶ
    + 例: I drink beer. We drink wine.
    + 例: I guzzle beer. We guzzle wine.
    + これでわかることは、「drink」と「guzzle」は同じ意味を持つ単語と予想できる
+ 共起行列
    + 分布仮説を利用して、単語をベクトル表現するとしたら、周囲の単語を「カウント」すれば良いと考えられる 
    + その周囲にどのような単語がどれだけ現れるのかをカウント し、それを集計するのです
    + ベクトル表現をするとこのようになる
    + それを全ての単語で実行し、テーブル上に並べたもの共起行列と呼ぶ
```
[you] -> [0, 1, 0, 0, 0, 0, 0]
[say] -> [1, 0, 1, 0, 1, 1, 0]
etc
``` 

+ ベクトルの間の類似度
    + 共起行列によって、単語をベクトル表現することができたため、次はこれを用いて、類似度計算をすることができる
    + ベクトル間の類似度を計算する手法が存在する
        + ベクトル内積
        + ユークリッド距離
        + コサイン類似度(直感的に2つのベクトルがどれだけ同じ方向を向いているか)
    + 今回は、コサイン類似度を用いるが、ゼロベクトルが入ってしまうと「０除算」が発生してしまう(eps=1e-8 が設定される) 
### カウントベース手法改善
+ 相互情報量
    + 共起行列の要素は、2つの単語が共起した回数を表しているが、これでは高頻度に出現する単語しか目が向けられないと言う結果になってしまう
    + 例：「the car」と言うフレーズはかなり使われていたとする。そのため、共起する回数が多くなってしまう。
    + 例：「the」と「drive」という単語には明確に強い関連性があると言える。しかし、出現頻度としては、「drive」よりも「the」の邦画多いため、「the」と言う単語が「car」と強い関連性を持つ単語だと学習されてしまう
    + そのため、相互情報量を用いることでこの問題の解決を図る
    + しかし、コーパスの語彙の数が多ければ単語のベクトル次元数も増大してしまう
    + さらに、ベクトルの中身をみると0が多く重要な情報がないと言える
```
x,yは確率変数
PMI(x,y) = log2(P(x,y)/P(x)P(y))
P(x)とP(y)は、xとyが起こる確率
P(x,y)は、xとyが同時に起こる確率
PMIの値が高いほど関連性が高い
```
+ 次元削減
    + ベクトルの次元数の増大、ベクトルの中身が重要な情報がないそういった問題を解決する手法が、次元削減である
    + 内容としては、データの軸となる部分を見つける
    + 大切な点としては、データとしての広がりを考慮した軸をとることで、1 次元の値だけでもデータの本質的な差異を捉えられるということです。これと同じようなことは、さらに多次元のデータでも行え ます。
    + ベクトル中のほとんどの要素が0である行列　-> 「疎のベクトル」
    + 疎なベクト ルから重要な軸を見つけて、より少ない次元で表現し直すことが次元削減に求められていること
    + そして、ほとんどの要素が0でない行列 -> 「密なベクトル」
    + 「密なベクトル」が我々が求めている単語の分散表現
    + 特異値分解
        + 任意の行列を3つの行列の積に分解する
        + X = USV^t と言う式になる
        + UとVは直行行列、Sは対角行列(対角成分以外は0)
+ SVDによる次元削減
    + 
+ PTBデータセット
+ PTBデータセットでの評価
### まとめ

## 第三章
### 推論ベースの手法とニューラルネットワーク
### シンプルなword2vec
### 学習データの準備
### CBOW モデルの実装
### word2vecに関する補足
### まとめ

## 第四章
### word2vecの改良1
### word2vecの改良2
### 改良版word2vecの学習
### word2vecに関する残りのテーマ
### まとめ

## 第五章
### 確率と言語モデル
### RNNとは
### RNNの実装
### 時系列データを扱うレイヤの実装
### RNNLMの学習と評価
### まとめ

## 第六章
### RNNの問題点
### 勾配消失とLSTM
### LSTMの実装
### LSTMを使った言語モデル
### RNNLMのさらなる改善
### まとめ

## 第七章
### 言語モデルを使った文章生成
### seq2seq
### seq2seqの実装
### seq2seqの改良
### seq2seqを用いたアプリケーション
### まとめ

## 第八章
### Attentionの仕組み
### Attention付きseq2seqの実装
### Attentionの評価
### Attentionに関する残りのテーマ
### Attentionの応用
### まとめ

